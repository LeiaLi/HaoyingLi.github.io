<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-svg" href="./images/nvidia.svg">
    <script src="https://unpkg.com/typewriter-effect@latest/dist/core.js"></script>
    <script src="https://kit.fontawesome.com/8290b48404.js" crossorigin="anonymous"></script>
    <link
    rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"
  />
    <link rel="stylesheet" href="./dist/style.css">

    <title>Adaptive Window Pruning for Efficient Local Motion Deblurring</title>
</head>
<body>
    <div class="main container">
        <!-- <nav class="menu">
            <ul>
                <li><a href="#news">News</a></li>
                <li><a href="https://arxiv.org/abs/2112.07658">Paper</a></li>
            </ul>
        </nav> -->

        <div class="wrapper">
            <div class="wrapper-title">
                <div id="title">Adaptive Window Pruning for Efficient Local Motion Deblurring</div>
            </div>

            <div class="wrapper-crew">
                <div class="crew">
                    <ul>
                        <li class="wow animate__animated animate__fadeInDown"><a target="_blank" class="" href="https://leiali.github.io/">Haoying Li</a></li>
                        <li class="wow animate__animated animate__fadeInDown"><a target="_blank" class="" href="https://scholar.google.com/citations?user=0Z89rfUAAAAJ">Jixin Zhao</a></li>
                        <li class="wow animate__animated animate__fadeInDown"><a target="_blank" class="" href="https://shangchenzhou.com/">Shangchen Zhou</a></li>
                        <li class="wow animate__animated animate__fadeInUp"><a target="_blank" class="" href="https://ieeexplore.ieee.org/author/38081851500">Huajun Feng</a></li>
                        <li class="wow animate__animated animate__fadeInUp"><a target="_blank" class="" href="https://li-chongyi.github.io/">Chongyi Li</a></li>
                        <li class="wow animate__animated animate__fadeInUp"><a target="_blank" class="" href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a></li>
                    </ul>
                </div>
            </div>

            <div class="wrapper-main-download">
                <div class="main-download">
                    <figure>
                        <img src="./img_source/images/ntu.png" alt="">
                    </figure>
                    <h2>CVPR 2022</h2>
                    <img src="./img_source/images/zju.png" alt="" width="200">
                    <!-- <div class="download-btn">
                        <a class="wow animate__animated animate__lightSpeedInLeft" target="_blank" href="https://arxiv.org/abs/2112.07658"><i class="far fa-sticky-note"></i> Paper (ArXiv) </a>
                        <a class="wow animate__animated animate__lightSpeedInRight" target="_blank" href=""><i class="fas fa-file-code"></i> Code (to come)</a>
                    </div> -->
                </div>
            </div>

            <div class="wrapper-articles">
                <div class="article">

                </div>


                <!-- <div id="news" class="article">
                    <div class="title">
                        <span>News</span>
                    </div>
                    <div class="news-wrapper">
                        <ul>
                            <li><i class="fa-solid fa-rss"></i> [Jul 2022] Code is released under <a target="_blank" href="https://github.com/NVlabs/A-ViT">NVLabs</a>.</li>
                            <li><i class="fa-solid fa-rss"></i> [Mar 2022] Accpetance as <b> oral presentation</b>.</li>
                            <li><i class="fa-solid fa-rss"></i> [Mar 2022] Our paper has been accepted to <a target="_blank" href="https://cvpr2022.thecvf.com/">CVPR 2022</a>.</li>
                        </ul>
                    </div>
                </div> -->

                <div class="article">
                    <div class="title">
                        <span>Abstract</span>
                    </div>
                    <article>
                        <figure class="article-image">
                        <!-- <img class="wow animate__animated animate__fadeInLeft" src="./images/client_server.png" alt=""> -->
                        <!-- <figcaption>*Equal contribution.**Equal advising.</figcaption> -->
                        <img class="wow animate__animated animate__fadeInLeft" src="./img_source/images/LMD.png" alt="">
                        </figure>

                        <p> We propose LMD-ViT, a Transformer-based local motion deblurring method with an adaptive window pruning mechanism. We prune unnecessary windows based on the predicted blurriness confidence supervised by our blur region annotation. In this process, the feature maps are pruned at varying levels of granularity within blocks of different resolutions (as depicted in the 3$^{rd}$\textasciitilde5$^{th}$ visualizations in Row 1). Unlike global deblurring methods 
                            that modify global regions (i.e. Uformer), LMD-ViT performs dense computing only on the active windows of blurry regions. Consequently, local blurs are efficiently removed without distorting sharp regions, as shown in Row 2.</p>
                    </article>
                    <!-- <figure class="article-image"></figure> -->
                </div>
                <div class="article">
                    <div class="title">
                        <span>Key Approach</span>
                    </div>
                    <article>
                        <p>
                        We reformulate Adaptive Computation Time (Graves'17) for this task, extending halting to discard redundant spatial tokens. The appealing architectural properties of vision transformers enables our adaptive token reduction mechanism to speed up inference without modifying the network architecture or inference hardware. We demonstrate that A-ViT requires no extra parameters or sub-network for halting, as we base the learning of adaptive halting on the original network parameters. We further introduce distributional prior regularization that stabilizes training compared to prior ACT approaches. On the image classification task (ImageNet1K), we show that our proposed A-ViT yields high efficacy in filtering informative spatial features and cutting down on the overall compute. The proposed method improves the throughput of DeiT-Tiny by 62% and DeiT-Small by 38% with only 0.3% accuracy drop, outperforming prior art by a large margin.
                        </p>
                    </article>

                    <figure class="article-image">
                        <img class="wow animate__animated animate__fadeInRight" src="./img_source/fig/dynamic_fig.png" alt="">
                        <figcaption>
                            Original image (left) and the dynamic token depth (right) of A-ViT-Tiny on the ImageNet-1K validation set. Distribution of token computation highly aligns with visual features. Tokens associated with informative regions are adaptively processed deeper, robust to repeating objects with complex backgrounds. Best viewed in color.
                        </figcaption>
                    </figure>

                    </figure>
                </div>
            </div>

            <div class="paper">
            <div id="paper" class="wrapper-extra-links">
                <div class="extra-link">
                    <div class="paper-pic">
                        <span class="paper-title">Paper</span>
                        <figure>
                            <a target="_blank" href="https://arxiv.org/abs/2112.07658"><img src="./img_source/fig/paper_cover.png" alt=""></a>
                        </figure>
                    </div>
                    <div class="description">
                        <p>
                            A-ViT: Adaptive Tokens for Efficient Vision Transformer, CVPR 2022.
                        </p>
                        <a target="_blank" href="https://arxiv.org/abs/2112.07658">paper</a>
                    </div>
                </div>
            </div>

           </div>

            <div class="wrapper-code">
                <span class="title">Bibtex</span>
                    <pre>
    <code id="code">

    @inproceedings{yin2022avit,
        title={{A}-{V}i{T}: {A}daptive Tokens for Efficient Vision Transformer},
        author={Yin, Hongxu and Vahdat, Arash and Alvarez, Jose and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        year={2022}
    }

    or

    @inproceedings{yin2022avit,
        title={{A}da{V}i{T}: {A}daptive Tokens for Efficient Vision Transformer},
        author={Yin, Hongxu and Vahdat, Arash and Alvarez, Jose and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
        booktitle={arXiv preprint arXiv:2112.07658},
        year={2021}
    }

                        </code>
                    </pre>
            </div>
        </div>
    </div>

    <script src="./dist/js/wow.min.js"></script>
    <script src="./dist/js/main.js"></script>
</body>
</html>
